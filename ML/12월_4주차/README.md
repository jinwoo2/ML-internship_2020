
... 
이번 주에 DQN을 공부하고 있던 도중 Reinforce 알고리즘을 먼저 알아야 DQN을 짤 수 있다는 것을 알게 됬다.
무슨 말이냐면 REINFORCE알고리즘(한 에피소드 마다 학습)의 발전된 형태로 Actor-Critic( 타임스텝마다 학습이 이루어짐)라서 그렇다.


우선 DQN을 공부했을때 딥살사에서 마찬가지로 오차함수를 MSE[(정답- 예측)^2]를 써서 오차함수를 최소화 하는 방향으로 인공신경망이 업데이트
된다는 것은 파악을 하였다.

그리고 replay 메모리방식으로 에이전트가 학습을 해서 점점 높은 점수를 받게 되면 더 좋은 샘플들이 리플레이 메모리에 저장된다. 

딥살사에서 on-policy방식에서의 단점인 '안 좋은 상황에 빠려버리면 탈출할 수 없는 상황이 존재한다' 를 DQN에서는 해결할 수 있다.

매 타임스텝마다 인공신경망을 업데이트 한다면, 안 좋은 상황에서 계속적으로 그 안 좋은 상황에 갖혀 벗어날 수 없다는 말이다.

그렇기에 임의의 공간에 여러 sample들을 저장해 놓고 무작위로 sample 추출을 하여 서로 시간적인 상관관계를 없엘 수 있기에 가능하다는 말
인 것 같다.

1. 현재 정책으로부터 발생된 상황(에이전트의 선택으로 발생된 상태)
2. 이전의 정책으로 발생한 상황(이전의 선택들로부터 발생한 상태)



Actor 네트워크와 Critic 네트워크라는 두 개의 네트워크를 사용합니다.

Actor는 상태가 주어졌을 때 행동을 결정하고, Critic은 상태의 가치를 평가합니다.

 Actor-Critic 은 Replay Buffer 를 사용하지 않고, 매 step 마다 얻어진 상태(s), 행동(a), 보상(r), 다음 상태(s’)를 이용해서 모델을 학습시킵니다.
