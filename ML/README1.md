핵심 개념

1. state , action, reward, agent, policy

St에서 a_t를 취하면, 다음 상태인 St+1로 가고 보상 r_t+1을 받는다.
agent는 action을 결정하고, 환경은 상태와 보상을 결정한다. 이때 환경은 MDP에 따라 다음 상태와 보상을 결정한다.

MDP는 Markov Decisino Process로 action을 취하고 환경에 의해 상태가 확률적으로 결정된다(어떤식으로 행동을 취할것인가에 대한 정책 결정)
강화학습의 기본 목표는 보상을 최대화 하는게 아닌 '누적보상'을 최대화 하는 것이 목표이다. 이 누적보상을 어떠한 규칙으로 만드는가가 기계학습에서 말하는 policy(정책)이다. 

강화학습은 마르코프 성질을 만족한다는 전제하에 동작한다. 따라서 Markov property를 근사하게라도 만족하는 문제에 국한하거나, 근사하게 만족할 수 있도록 상태 표현을 설계해서 적용한다.
S=맑은후맑음,비온후맑음,눈온후맑음,⋯,눈온후눈
등으로 특징을 섞어 표현하면 마르코프 성질에 근사하게 표현할 수 있다. MDP는 위와 같은 표현들로 (다음 상태와 보상)이 확률적으로 표현하는 것과 (상태, 보상)이 한가지 있고 나머지는 (0,0)
으로 표현하는 확률론적 MDP, 결정론적 MDP 두가지로 나뉠 수 있다.

이때 상태, 행동, 보상은 연속적이거나 이산적이다. (이후에 상태가 연속구간이라면, 가치함수도 연속구간에서 표현 가능하다 -> 가치함수의 함수근사, 정책함수의 근사)



policy, 정책이란 π(a|s)=P(a|s),∀s,∀a 모든 상태 S에 대해서 action을 취할 모든 확률을 파이함수로 표현한 것이다. 이 정책 함수가 강화학습의 효율을 결정한다. 많은 상태를 만들고 그 
상태에서 취할 수 있는 액션의 모든 확률을 구한다면 그 때에 우리는 최선의 결정을 할 수 있다. 하지만 바둑이나 사람들끼리 즐기는 n:n 대결 게임같은 무한한 경우의 수를 가지고 있다면
모든 상태를 찾는 다는 것은 불가능 할 것이다. 그래서 우리는 상태 공간을 일일이 찾는 대신에 '가치함수'를 이용하여 최적의 정책 함수를 찾는다.

이제 강화학습의 사이클을 생각해보면 1.정책을 시행해보고, 2.그 정책을 평가한다. 3. 그리고 평가된 정책을 개선한다. 4. 1. 정책을 시행한다 재귀적으로 돌면서 좋은 정책을 찾는 것이 
강화학습의 한 사이클이다. 

이때 어떠한 상태에서 policy를 따라 갔을때 return의 기대값의 총합을 가치함수라 한다. 쉽게 말하자면 좋은 정책을 찾는 함수를 가치함수라고 한다. 조금 더 쉽게 말하자면 특정 정책의 
좋은 정도를 대신 평가해 주는 게 가치함수이다. 이러한 가치 함수에는 2가지가 있다. 상태 가치함수와 행동 가치함수이다. 

1) 상태 가치 함수
  에이전트가 어떠한 행동을 수행하면서 상태가 시간에 따라 변하게 된다. 이 때 보상을 받게 되고 시간에 따라 할인된 보상을 더해서 얻게되는 가치를 표현한 함수이다...
  좀 더 쉽게 말하자면 현재 상태의 좋고/나쁨을 표현한다. 상태 가치 함수를 수학적으로 표현하면 아래와 같다.
  ![캡처](https://user-images.githubusercontent.com/38103094/95785273-6c260180-0d10-11eb-8ff1-cf455dec3bc6.PNG)
  식의 해설을 좀 붙이자면 어떤 시간 t에서 전략 \pi를 따를때 기대되는(E_\pi-nondeterministic일 경우 평균-) 어떤 상태 s(V_pi(s), S_t=s) 의 가치는 
  미래의 보상들의 총합(R_{t+1}+{\gamma}R_{t+2}+{\gamma}^{2}R_{t+3}+...)으로 표현된다
  gamma는 보통 0에서 1의 값을 부여한다. 만약 gamma가 0 이라면 오직 다음 시간(t+1)의 보상만을 고려한다. 이때의 장점은 빨리 최적의 행동을 결정할 수 있다는 점이다. 
  gamma가 1이라면 미래의 보상도 바로 다음의 보상만큼 중요하게 생각한다. 이 경우, 당장의 보상은 최대화 할 수 없지만 미래의 수까지 내다보면서 행동을 할 수 있다는 장점이 있다.
  실제 상황에서는 문제에 따라 최적의 gamma이 다르고, 실험을 통해 최적의 gamma을 설정해 주어야 한다.(알파고가 학습했을 때에는 1로 두고 했다고 한다..)
  
  
2) 행동 가치 함수
 ![캡처1](https://user-images.githubusercontent.com/38103094/95785596-0128fa80-0d11-11eb-9563-c43daa42b48c.PNG)
 현재 t일때 policy를 따를 때 기대되는 어떤상태 St에서 어떤 행동 a_t를 했을때의 가치(Q)는 미래 보상들의 총합이다.
 상태 가치 함수에서 액션에 대한 조건이 추가된 것이다.
 
 위 두가지 가치함수를 보았다.


벨만 방정식은 현재 상태와 다음 상태의 관계를 나타내는 방정식입니다. 두가지 종류가 존재합니다.

벨만 기대 방정식 : 반복적으로 기대값을 업데이트하기 위해서, 현재와 다음의 가치함수 사이 관계를 정의합니다. 특정 정책일 때의 가치함수 사이의 관계를 의미합니다. 모든 상태에 대한 가치함수를 반복적으로 업데이트하여 참 가치함수값이 나옵니다. 현 정책에 대한 수렴값을 구할 수 있습니다.
벨만 최적 방정식 : 가장 큰 가치함수를 구하는 정책을 최적 정책이라고 합니다. 벨만 기대 방정식에 맞춰 더 좋은 정책을 찾아내면 해당 정책이 바로 최적의 정책입니다. 이때, 최적의 가치함수 값을 내는 것을 최적 가치함수라고 하며, 그 때의 정책이 최적 정책입니다. 이때 가치함수들 사이의 관계식을 벨만 최적 방정식이라고 합니다.


출처: https://davinci-ai.tistory.com/31 [DAVINCI - AI]





쉽게 설명된 정의 

상태(State) : 정적인 요소 + 동적인 요소를 의미합니다.
행동(Action) : 어떠한 상태에서 취할 수 있는 행동을 의미합니다.
Stochastic transition model : 어떤 상태에서 특정 행동을 하여 다음 상태에 도달할 확률
보상(Reward) : Agent가 학습할 수 있는 유일한 정보를 의미합니다. 어떤 상태에서 행동을 하여 다음 상태가 되고, 이때 받는 보상값은 다음 상태가 되는 것에 대한 보상입니다.
정책(Policy) : 순차적 행동 결정 문제(MDP)에서 구해야할 답을 의미합니다. 모든 상태에 대해 Agent가 어떠한 Action을 해야 하는지 정해놓은 것을 의미합니다.


출처: https://davinci-ai.tistory.com/31 [DAVINCI - AI]
