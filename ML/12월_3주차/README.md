
12월 2주차는 기말고사 시험이였다.

![캡처](https://user-images.githubusercontent.com/38103094/102561458-24f52c00-4118-11eb-964a-545f93b1cd22.PNG)

환경 먼저 세팅을 해준다

Cart-Pole의 상태는 4차원 벡터(배열)로 각 값은 다음과 같은 의미를 갖는다는 것을 잊지 말자
환경 변수
  1. x : Cart의 가로상의 위치
  2. θ : Pole의 각도
  3. dx/dt : Cart의 속도
  4. dθ/dt : θ의 각속도

게임이 끝나는 상황:

1. θ가 15˚이상이 되었을 떄,
2. 원점으로부터의 x의 거리가 2.4이상이 되었을 떄.

에이전트가 취할 수 있는 행동은 다음 두가지 이다

1. left 이동
2. right 이동


지금까지 위에 경우의 수들로 아래와 같은 Q-table을 만드는 것은 어렵다

![캡처](https://user-images.githubusercontent.com/38103094/102559565-81a21800-4113-11eb-877a-8e8841902ddd.PNG)


위 표는 각 상태마다 어떤 행동을 할지 '정해논 것이다'
액션은 left, right 2개이니 괜찮은데 상태는 무수히 많아진다...

그러니 신경망으로 만들어서 cart-pole을 해결해야 한다.

![캡처](https://user-images.githubusercontent.com/38103094/102561491-39d1bf80-4118-11eb-978f-eaa5503281ad.PNG)

4개의 입력층과 2개의 출력층 을 가진 신경망을 생각해보자.
이 신경망을 Q라 한다. Q(s, a)에서 4개의 입력층은 s를 대신하고, 2개의 출력층은 a를 대신한다. 그리고 각 출력층의 값은 Q(s, a)의 값이 된다.
(layer.Dense층은 fully-connected 층이라고 한다)
      노드의 개수(뉴런 수) 24개  input_dim = 4 ----> 4개의 입력 , 활성함수는 relu  ----------1번째 층   
      3번째 층에는 활성화 함수를 'linear'로 하였다.  디폴트 값으로 입력뉴런과 가중치로 계산된 결과값이 그대로 출력으로 나오게 하는 것이다
      
![캡처](https://user-images.githubusercontent.com/38103094/102563240-2a547580-411c-11eb-9e85-e675ea1ed3cc.PNG)

score에는 각 에피소드가 끝났을 때 점수를 담는 배열이고,
memory는 현재 상태와 행동, 다음상태, 보상 등을 보관하는 배열이다




