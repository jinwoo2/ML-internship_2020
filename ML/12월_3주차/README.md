
시작에 앞서 cartpole 환경 먼저 세팅을 해준다

![캡처](https://user-images.githubusercontent.com/38103094/102561458-24f52c00-4118-11eb-964a-545f93b1cd22.PNG)


Cart-Pole의 상태는 4차원 벡터(배열)로 각 값은 다음과 같은 의미를 갖는다는 것을 잊지 말자
환경 변수
  1. x : Cart의 가로상의 위치
  2. θ : Pole의 각도
  3. dx/dt : Cart의 속도
  4. dθ/dt : θ의 각속도

게임이 끝나는 상황:

1. θ가 15˚이상이 되었을 떄,
2. 원점으로부터의 x의 거리가 2.4이상이 되었을 떄.

에이전트가 취할 수 있는 행동은 다음 두가지 이다

1. left 이동
2. right 이동


지금까지 위에 경우의 수들로 아래와 같은 Q-table을 만드는 것은 어렵다

![캡처](https://user-images.githubusercontent.com/38103094/102559565-81a21800-4113-11eb-877a-8e8841902ddd.PNG)


위 표는 각 상태마다 어떤 행동을 할지 '정해논 것이다'
액션은 left, right 2개이니 괜찮은데 상태는 무수히 많아진다...

그러니 신경망으로 만들어서 cart-pole을 해결해야 한다.

![캡처](https://user-images.githubusercontent.com/38103094/102561491-39d1bf80-4118-11eb-978f-eaa5503281ad.PNG)

4개의 입력층과 2개의 출력층 을 가진 신경망을 생각해보자.
이 신경망을 Q라 한다. Q(s, a)에서 4개의 입력층은 s를 대신하고, 2개의 출력층은 a를 대신한다. 그리고 각 출력층의 값은 Q(s, a)의 값이 된다.
(layer.Dense층은 fully-connected 층이라고 한다)
      노드의 개수(뉴런 수) 24개  input_dim = 4 ----> 4개의 입력 , 활성함수는 relu  ----------1번째 층   
      3번째 층에는 활성화 함수를 'linear'로 하였다.  디폴트 값으로 입력뉴런과 가중치로 계산된 결과값이 그대로 출력으로 나오게 하는 것이다
      
![캡처](https://user-images.githubusercontent.com/38103094/102563240-2a547580-411c-11eb-9e85-e675ea1ed3cc.PNG)

score에는 각 에피소드가 끝났을 때 점수를 담는 배열이고,
memory는 현재 상태와 행동, 다음상태, 보상 등을 보관하는 배열이다


ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
에러 해결이 안되서 cart pole 문제 대신 Grid World로  다시 구현하기로 하였다


장애물은 세 개이고, 같은 속도와 방향으로 움직인다.
속도가 같다는 것은 한 스텝마다 한 칸씩 움직인 다는 것
벽에 부딪힐 경우 다시 튕겨져 나와 반대방향으로 움직임

에이전트가 장애물을 만나면 -1 보상
           도착했을 때   +1 보상

문제에서 정의하는 상태
1. 에이전트에 대한 도착지점의 상대 위치 x,y
2. 도착지점의 라벨
3. 에이전트에 대한 장애물의 상대 위치 x,y
4. 장애물의 라벨
5. 장애물의 속도

지금 이거를 ‘특징추출을 하였다’라고 말할 수 있다. 강화학습에서는 이렇게 특징을 추출해야 한다. 

살사의 큐 함수 update식 

![수식 5 6](https://user-images.githubusercontent.com/38103094/103357075-a922b880-4af5-11eb-8b87-485bcb0f2db6.jpg)

-현재 상태에서 행동을 선택해서 다음 상태로 가고 다음 상태에서 다음 행동을 정하면 위 수식 이용

딥살사는 table형태의 강화학습이 아님 -> 경사하강법을 이용한 인공신경망 update이다

오차함수를 먼저 정의 -> MSE

중요한 부분은 강화학습에서는 정답이 없다. 하지만 위 수식에서는 정답 역할을 하는 부분과 예측에 해당하는 것이 나뉘어 있다.


살사의 큐함수 업데이트 식에서 정답역할

![수식5 7](https://user-images.githubusercontent.com/38103094/103357081-aaec7c00-4af5-11eb-914c-13eda29a75c5.jpg)

살사의 큐함수 업데이트 식에서 예측역할

![수식 5 8](https://user-images.githubusercontent.com/38103094/103357083-ac1da900-4af5-11eb-9484-e29ff3fe5dcc.jpg)

이 정답과 예측역할하는 함수를 MSE식에 집어 넣어서 오차함수를 만드는 것이다.
딥살사의 오차함수 식은 아래와 같다

![수식5 9](https://user-images.githubusercontent.com/38103094/103357088-ade76c80-4af5-11eb-9aa0-6ccb6935cf15.jpg)

메인 루프 if __name__ == "__main__":
  에이전트와 환경이 작용하는 부분이다. 환경이 무엇이고, 에이전트가 무엇인지 정의를 먼저    해야 된다
  코드-00

 에이전트 클래스를 구성할 때에 무엇이 필요한지를 알기 위해서는 에이전트가 환경과 어떻게 액션을 취하고 행동하는지 알아야 된다. 
 
1. 상태에 따른 행동 선택

2. 선택한 행동으로 환경에서 한 타임스텝을 진행

3. 환경으로부터 다음 상태와 보상을 받음

4. 다음 상태에 대한 행동을 선택

5. 환경으로부터 받은 정보를 토대로 학습을 진행


이 부분에 대한 코드는 아래와 같다

에이전트는 다음상태 next_state에 대한 행동을 정하고 샘플( s a r s' a' = sate, action, reward, next_state, next_action)을 가지고 학습을 진행한다.

기존의 강화학습 알고리즘 

  - 상태에서 행동을 선택하기 위해 에이전트는 큐함수 테이블 이용

딥살사 알고리즘

  - 인공신경망으로 현재 상태의 특징들이 인공신경망의 입력으로 들어가고 인공신경망은 각       행동에 대한 큐함수를 출력해준다. (이는 근사치이다.)

따라서 에이전트 클래스는 상태(input) 행동에 대한 큐함수(output)인 인공신경망모델을 갖는다.

![그림5 24](https://user-images.githubusercontent.com/38103094/103357096-af189980-4af5-11eb-8f22-482ffb097c70.jpg)

우리는 input의 개수를 알 수 있다. 문제상태를 정의할 때 위치(x,y)와 라벨정보를 체크할 때 
장애물의 개수가 3개이므로 장애물의 위치정보, 라벨정보, 장애물 속도 까지 3*4 =12개를 가지고 기존의 에이전트의 위치(x,y)와 도착지점 라벨까지 총 15개의 정보를 가진다. 
그러므로 input의 개수가 15개로 정해졌고 은닉층은 kears dense층 2개  그리고 출력층은 선택가능한 행동의 개수 5개로 만들었다.  


self.model = self.build_model()
->build_model을 통해 얻은 model은 에이전트 내부에서 self.model이라는 변수로 사용 가능 모델을 으용해서 큐함수의 값을 얻을 수 있음. 큐함수를 통해 에이전트는 행동을 선택할 수 있는데 이제 get_action함수가 그 역할을 한다.

get_action함수 스크린샷


q_values = self.model.predict(state)
-> predict의 출력은 이중배열이다. [ [], [], [] ,[] ] 이런식이다. 그렇기에 [] , [], [], [] 와 같이 변경하기 위해 [0]을 붙인거다. 이 값은 현재 상태 state에 대한 큐함수의 값이 된다. 탐욕 정책일 때 get_action함수는 큐함수의 값 중에서 가장 큰 값을 가지는 행동을 반환한다.

MSE = (정답 - 예측)^2  = 
수식 5.9

샘플을 가지고 인공신경망을 업데이트하는 함수의 이름은 train_model이며, 아래와 같다.




지금 구현한 살사코드는 ε -탐용정책에서 사용되는 ε(epsilon)은 시간에 따라서 감소시킨다.

왜냐하면 초반에는 에이전트가 탐험을 통해 다양한 상황에 대해서 학습하고 충분히 학습이 된 이루어진 이후에는 예측하는 대로 에이전트를 움직이기 위해서이다.

코드에서 상태값이 float32이다. 이는 케라스모델에서 들어가는 입력이 float형식이여야 하기 때문에 np.float를 이용해 자료형을 변형시켰다.

추가로 책에 설명된 부분)
이번 살사의 출력은 5개의 큐함수 값을 가진다. 하지만 모델을 update하기 위해 오차함수를 계산할 출력은 이 가운데 실제로 행동이 된 하나의 큐함수 뿐이다. 따라서 1. 타겟에서 실제 행동에 해당하는 큐함수 이외에는 예측 값에서 해당하는 큐함수 값과 동일해야 한다. 그렇기에 코드 구현에 있어서 1. 예측을 타깃이라고 놓고 2.실제 행동에 해당하는 부분을 뒤에서 계산하여 변경하면 실제 행동에 대한 큐함수를 제외하고 타깃과 예측의 차이가 0이 된다.

1.

2.

(타겟에서 실제 행동에 해당하는 부분을 계산하는 코드)

3. 계산한 타겟과 상태 입력을 통해 인공신경망 update (* 계산한 타겟을 다시 model.predict(state)를 통해 출력되는 형태로 변형해야 함.

4. model.fit 함수를 싱행하여 자동으로 설정한 오류함수를 감소하는 방향으로 한번 인공신경망을 update한다.




인공신경망은 이 오차함수를 통해 업데이트를 하는 것이 학습을 하는 것이다. 케라스에서는 오차함수만 정의가 되 있다면 간단하게 업데이트를 할 수 있다.



상태가치 함수는 어떤 상태가 좋은 상태인지 알려주지만 상태로 가는 방법인 행동에 대해서는 평가x, 따라서 가치함수를 구하고 높은 가치를 가지는 상태로 가기위한 행동은 따로 고려해야함. -> 이를 동시에 하기 위해 상태와 행동을 동시에 고려한 Q-Function (action-value function)을 사용함.

###실제로 돌아가는 결과물이다

![KakaoTalk_20210101_014327923](https://user-images.githubusercontent.com/38103094/103418646-ec9f2480-4bd2-11eb-8685-a193310e0d50.jpg)


![KakaoTalk_20210101_014342521](https://user-images.githubusercontent.com/38103094/103418647-ee68e800-4bd2-11eb-80ab-055b10cd01ba.jpg)


![KakaoTalk_20210101_014355491](https://user-images.githubusercontent.com/38103094/103418649-ef9a1500-4bd2-11eb-9665-b9a68384def2.jpg)
