핵심 개념

1. state , action, reward, agent, policy

St에서 a_t를 취하면, 다음 상태인 St+1로 가고 보상 r_t+1을 받는다.
agent는 action을 결정하고, 환경은 상태와 보상을 결정한다. 이때 환경은 MDP에 따라 다음 상태와 보상을 결정한다.

MDP는 Markov Decisino Process로 action을 취하고 환경에 의해 상태가 확률적으로 결정된다(어떤식으로 행동을 취할것인가에 대한 정책 결정)
강화학습의 기본 목표는 보상을 최대화 하는게 아닌 '누적보상'을 최대화 하는 것이 목표이다. 이 누적보상을 어떠한 규칙으로 만드는가가 기계학습에서 말하는 policy(정책)이다. 

강화학습은 마르코프 성질을 만족한다는 전제하에 동작한다. 따라서 Markov property를 근사하게라도 만족하는 문제에 국한하거나, 근사하게 만족할 수 있도록 상태 표현을 설계해서 적용한다.
S=맑은후맑음,비온후맑음,눈온후맑음,⋯,눈온후눈
등으로 특징을 섞어 표현하면 마르코프 성질에 근사하게 표현할 수 있다. MDP는 위와 같은 표현들로 (다음 상태와 보상)이 확률적으로 표현하는 것과 (상태, 보상)이 한가지 있고 나머지는 (0,0)
으로 표현하는 확률론적 MDP, 결정론적 MDP 두가지로 나뉠 수 있다.

이때 상태, 행동, 보상은 연속적이거나 이산적이다. (이후에 상태가 연속구간이라면, 가치함수도 연속구간에서 표현 가능하다 -> 가치함수의 함수근사, 정책함수의 근사)



policy, 정책이란 π(a|s)=P(a|s),∀s,∀a 모든 상태 S에 대해서 action을 취할 모든 확률을 파이함수로 표현한 것이다. 이 정책 함수가 강화학습의 효율을 결정한다. 많은 상태를 만들고 그 
상태에서 취할 수 있는 액션의 모든 확률을 구한다면 그 때에 우리는 최선의 결정을 할 수 있다. 하지만 바둑이나 사람들끼리 즐기는 n:n 대결 게임같은 무한한 경우의 수를 가지고 있다면
모든 상태를 찾는 다는 것은 불가능 할 것이다. 그래서 우리는 상태 공간을 일일이 찾는 대신에 '가치함수'를 이용하여 최적의 정책 함수를 찾는다.

이제 강화학습의 사이클을 생각해보면 1.정책을 시행해보고, 2.그 정책을 평가한다. 3. 그리고 평가된 정책을 개선한다. 4. 1. 정책을 시행한다 재귀적으로 돌면서 좋은 정책을 찾는 것이 
강화학습의 한 사이클이다.  -----------------------------------------------------------------------------------ㄱ

이때 어떠한 상태에서 policy를 따라 갔을때 return의 기대값의 총합을 가치함수라 한다. 쉽게 말하자면 좋은 정책을 찾는 함수를 가치함수라고 한다. 조금 더 쉽게 말하자면 특정 정책의 
좋은 정도를 대신 평가해 주는 게 가치함수이다. 이러한 가치 함수에는 2가지가 있다. 상태 가치함수와 행동 가치함수이다. 

1) 상태 가치 함수
  에이전트가 어떠한 행동을 수행하면서 상태가 시간에 따라 변하게 된다. 이 때 보상을 받게 되고 시간에 따라 할인된 보상을 더해서 얻게되는 가치를 표현한 함수이다...
  좀 더 쉽게 말하자면 현재 상태의 좋고/나쁨을 표현한다. 상태 가치 함수를 수학적으로 표현하면 아래와 같다.
  ![캡처](https://user-images.githubusercontent.com/38103094/95785273-6c260180-0d10-11eb-8ff1-cf455dec3bc6.PNG)
  식의 해설을 좀 붙이자면 어떤 시간 t에서 전략 \pi를 따를때 기대되는(E_\pi-nondeterministic일 경우 평균-) 어떤 상태 s(V_pi(s), S_t=s) 의 가치는 
  미래의 보상들의 총합(R_{t+1}+{\gamma}R_{t+2}+{\gamma}^{2}R_{t+3}+...)으로 표현된다
  gamma는 보통 0에서 1의 값을 부여한다. 만약 gamma가 0 이라면 오직 다음 시간(t+1)의 보상만을 고려한다. 이때의 장점은 빨리 최적의 행동을 결정할 수 있다는 점이다. 
  gamma가 1이라면 미래의 보상도 바로 다음의 보상만큼 중요하게 생각한다. 이 경우, 당장의 보상은 최대화 할 수 없지만 미래의 수까지 내다보면서 행동을 할 수 있다는 장점이 있다.
  실제 상황에서는 문제에 따라 최적의 gamma이 다르고, 실험을 통해 최적의 gamma을 설정해 주어야 한다.(알파고가 학습했을 때에는 1로 두고 했다고 한다..)
  
  
2) 행동 가치 함수
 ![캡처1](https://user-images.githubusercontent.com/38103094/95785596-0128fa80-0d11-11eb-9563-c43daa42b48c.PNG)
 현재 t일때 policy를 따를 때 기대되는 어떤상태 St에서 어떤 행동 a_t를 했을때의 가치(Q)는 미래 보상들의 총합이다.
 상태 가치 함수에서 액션에 대한 조건이 추가된 것이다.
 
 위에서 두가지 가치함수를 보았다.
이 두 가지 가치함수를 통해서 정책을 최적화 시키는데 바로 벨만 방정식을 사용한다. 
![캡처2](https://user-images.githubusercontent.com/38103094/95786321-3f72e980-0d12-11eb-831e-7484675520ae.PNG)
출처: https://davinci-ai.tistory.com/31 [DAVINCI - AI]
위 그림을 보면 벨만 방정식을 사용하여 각각 상태에 대한 가치함수 기대값을 얻을 수 있습니다. 이것들을 모두 계산하여 최적의 정책을 찾는 것이다.
벨만 방정식은 현재 상태와 다음 상태의 관계를 나타내는 방정식이고 두가지 종류가 존재한다.

벨만 기대 방정식 : 반복적으로 기대값을 업데이트하기 위해서, 현재와 다음의 가치함수 사이 관계를 정의합니다. 특정 정책일 때의 가치함수 사이의 관계를 의미합니다. 모든 상태에 대한 가치함수를 반복적으로 업데이트하여 참 가치함수값이 나옵니다. 현 정책에 대한 수렴값을 구할 수 있습니다.
벨만 최적 방정식 : 가장 큰 가치함수를 구하는 정책을 최적 정책이라고 합니다. 벨만 기대 방정식에 맞춰 더 좋은 정책을 찾아내면 해당 정책이 바로 최적의 정책입니다. 이때, 최적의 가치함수 값을 내는 것을 최적 가치함수라고 하며, 그 때의 정책이 최적 정책입니다. 이때 가치함수들 사이의 관계식을 벨만 최적 방정식이라고 합니다. 

ㄱ에서 우리는 강화학습의 정책을 찾는 사이클을 보았다 여기서 벨만 기대 방정식으로 현재와 다음 가치함수와의 관계를 찾아 정책의 수렴값을 구하고 벨만 최적 방정식을 통해 max 가치함수를 
찾아내어 max가치함수에서 정책에 대한 수렴값을 찾는 것이다. 다이나믹 프로그래밍(Dynamic Programming, DP)으로 작은 문제들로 쪼개서 풀게 됩니다. 각 상태에서의 가치함수를 구하고 업데이트를 하고, 업데이트 이후의 가치함수를 다시 구하는 방식으로 반복하는 사이클이다.

이렇게만 하면 강화학습이 끝인가? 아니다 DP에는 치명적인 단점이 있다.
동적 프로그래밍은 MDP의 상태 전이 확률과 보상 함수가 주어져야 풀 수 있다. 즉 환경 모델이 주어졌을 때에만 DP가 가능하다는 것이다. 그래서 현실적으로 상태 전이 확률과 보상 함수를 미리 알 수 없는 대부분의 현실 문제에서 젹용하기가 어렵다. 그리고 설령 문제가 정말 잘 정의가 되어 있어서 상태 전이 확률과 보상 함수를 알고 있더라도 상태의 수가 많아지면 현실적으로 벨만 방정식을 풀기 어려워 진다고 한다. 

그렇다면 환경 모델(s, a, s',a)이 없을때에는, 경우의 수가 너무 많을 때에는 정말 강화학습을 할 수 없는 것인가 에 대한 생각을 다시 한 번 고민하게 된다.

우선 현실 세계를 생각해보면 이산적인가 연속적인가 부터 생각해 봐야 한다. 현실은 이산적인가? 아니다 연속이다. 즉 상태가 연속적이다 라고 말 할 수 있다. 연속적인 상태에서, 환경 모델이
주어지지 않으 model-free에서 푸는 강화학습이 최근에서야 등장하고 있다. 예측과 제어를 통해서 가치함수를 조절하는 것이다.
약간 기계공학 과목에서 수치해석을 풀 때 느낌을 받았다.(제어란 말을 컴공에 와서 처음 보았기 때문이다. 자동제어나 수치해석에서 PID 제어나 제어시스템을 보는 느낌이였다.)

예측은 에이전트가 주어진 정책에 따라 환경과의 feedback을 통해 상태 가치 함수를 학습시키는 것이다. 강화 학습에서는 모든 상태에 대해서 가치를 판단할 수 없기 때문에 무한한 경우에서 몇가지 수를 집는 샘플링(sampling)을 한다. 에이전트는 이런 샘플들을 경험을 하면서 환경을 학습합니다. 제어는 예측을 통해 학습한 가치 함수를 기반으로 정책을 학습하는 것입니다.
이러한 예측에는 2가지가 있다.
1) 몬테 카를로 예측

![몬테 카를로](https://user-images.githubusercontent.com/38103094/95788846-4819ee80-0d17-11eb-9563-9d66a577ad27.PNG)
샘플링 -> 평균 -> 가치 함수 추정  [몬테 카를로 예측의 가치 함수 추정 과정]

계산이 아닌 예측은 여러번의 에피소드에서  s상태 방문해서 얻은 반환값들의 평균을 통해 가치함수를 추정한다.

하지만 몬테 카를로 예측에는 치명적인 단점이 있다. 하나의 에피소드를 반드시 s까지 가보고 반환되는 누적 보상을 통해 update를 한다는 것이다. 즉 실시간이 아니다.
그와 정 반대로 실시간으로 예측을 할 수 있는 것이 아래에서 소개되는 시간차 예측이다.
2) 시간차 예측
![시간차 예측1](https://user-images.githubusercontent.com/38103094/95788773-202a8b00-0d17-11eb-993c-a5f4fea59f4d.PNG)
![시간차 예측2](https://user-images.githubusercontent.com/38103094/95788776-228ce500-0d17-11eb-8a02-ea1a7b8f34d0.PNG)
시각 자료 출처 : https://www.slideshare.net/DongMinLee32/part-2-91522217
하나의 에피소드를 가지 않고 행동 a를 할 때마다 즉시 보상 r을 통해서 상태 가치 함수를 update하는 것이 시간차 예측이다.

하지만 시간차 예측에도 문제는 있다. 가치 함수를 현재 상태에 대해서만 update를 한다는 것이다. 지금까지는 정책평가과 정책 개선을 번갈아 가면서 가치함수가 최적의 가치함수로 수렴할 때까지 계산을 하였다. 

그래서 시간차 예측에서 별도의 policy를 두지 않고 가치함수가 주어진 상태에서 행동을 결정할 때 지금 현재 가장 큰 가치를 지는 행동을 취하는 탐욕 정책을 펼친다.
이를 greedy method라고 한다. 
![on-policy](https://user-images.githubusercontent.com/38103094/95789506-9e3b6180-0d18-11eb-8f37-d89d4cb63166.PNG)
환경 모델에서 샘플(S, a, r, St+1, a_t+1)[SARSA 5가지 구성요소의 스펠링을 다 붙여서 살사 알고리즘이라고 한다.]을 구성할 때에 탐욕 정책으로 최선의 선택을 하고 샘플로 접근하는 Q함수를 구성할 때에는 입실론-greedy  방식을 쓴다. -> 왜냐하면 처음 접근을 잘못하였는데 거기서는 최선인 탐욕 정책에 따라 잘못된 방향으로 쭉 진행하는 것을 방지하는 이유에서다. 이를 많은 사람들이 탐험(Exploration) 한다고 한다.  즉 살사에서 Q함수는 탐험과 탐욕을 둘 다 쓰는 방식을 사용한다. 

! 정리하자면 시간차 예측에서는 실시간으로 가치 함수만을 update하여(벨만 기대 방정식으로) 별도의 policy 없이 e-greedy + greedy 방식으로 Q함수를 update하여 액션을  선택한다.


그렇다면 살사 알고리즘이 괜찮은가? 아니다 환경 모델에서 a_t+1을 생각해 봐야 한다. 만일 e-greedy 방식으로 t+1시간에서 Q함수의 값이 낮아져 버린다면 에이전트가 다시 그 환경에 도착했을때 a_t+1(같은 행동)을 하지 않을 것이다. 하지만 전체적인 큰 그림에서는 설사 바로 앞에서는 별로인 길이더라도 반드시 그 길목을 지나가야만 최적의 값이 찾아진다면? 이라는 현상이 간혹 나올 수 있다. 이때 살사 알고리즘은 다른 액션의 q함수들이 낮아져서 다시 그 행동을 해야 하기에는 너무 많은 비용이 들어간다. 

이때에 현재 행동하는 정책과 독립적으로 학습하는 방법인 Q-learning 이 제시된다. 행동하는 정책은 행동하는 정책대로, 학습하는 정책은 학습하는 정책대로 , 정책을 2가지로 분리시킨 것이다. sars까지 진행한 수 e-greedy method에 따른 행동 A를 결정하는 것이 아니라 현재 상태에서 가장 큰 큐함수를 현재 큐함수로 update해버린다. 즉 다음상태에서 어떠한 행동을 했는지와 상광 없이 현재 상태 s의 큐함수를 업데이트 할 때에는 다음 상태의 가장 큰 큐함수로 update한다. 










쉽게 설명된 정의 

상태(State) : 정적인 요소 + 동적인 요소를 의미합니다.
행동(Action) : 어떠한 상태에서 취할 수 있는 행동을 의미합니다.
Stochastic transition model : 어떤 상태에서 특정 행동을 하여 다음 상태에 도달할 확률
보상(Reward) : Agent가 학습할 수 있는 유일한 정보를 의미합니다. 어떤 상태에서 행동을 하여 다음 상태가 되고, 이때 받는 보상값은 다음 상태가 되는 것에 대한 보상입니다.
정책(Policy) : 순차적 행동 결정 문제(MDP)에서 구해야할 답을 의미합니다. 모든 상태에 대해 Agent가 어떠한 Action을 해야 하는지 정해놓은 것을 의미합니다.

출처: https://davinci-ai.tistory.com/31 [DAVINCI - AI]
